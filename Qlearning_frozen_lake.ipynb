{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfFv6XD2nwCN","executionInfo":{"status":"ok","timestamp":1684170888756,"user_tz":-330,"elapsed":17357,"user":{"displayName":"PRIYANSH SRIVASTAVA (RA2011047010022)","userId":"02639638076610974849"}},"outputId":"748a2a29-56a6-4242-c4be-07b92beec75d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.48130663 0.45538179 0.40459107 0.4269724 ]\n"," [0.28364326 0.2972368  0.21366365 0.39359471]\n"," [0.31075468 0.27339823 0.26223331 0.28590036]\n"," [0.19141684 0.07169671 0.05037005 0.09195343]\n"," [0.49085176 0.30737112 0.38486896 0.29584442]\n"," [0.         0.         0.         0.        ]\n"," [0.18995486 0.18240083 0.12137764 0.18638361]\n"," [0.         0.         0.         0.        ]\n"," [0.30108276 0.29479794 0.35622174 0.53969676]\n"," [0.47245904 0.6311741  0.49260539 0.40020258]\n"," [0.60595428 0.38741066 0.33555638 0.25839696]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.25874135 0.55277729 0.74046285 0.58866823]\n"," [0.67387712 0.8852763  0.64569    0.74526304]\n"," [0.         0.         0.         0.        ]]\n"]}],"source":["import gym\n","import numpy as np\n","\n","# Create the FrozenLake environment\n","env = gym.make('FrozenLake-v1')\n","\n","# Set the parameters\n","num_episodes = 10000\n","learning_rate = 0.1\n","discount_factor = 0.99\n","epsilon = 0.1\n","\n","# Initialize the Q-table\n","state_space = env.observation_space.n\n","action_space = env.action_space.n\n","q_table = np.zeros((state_space, action_space))\n","\n","# Q-learning algorithm\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","\n","    while not done:\n","        if np.random.uniform(0, 1) < epsilon:\n","            # Explore: Choose a random action\n","            action = env.action_space.sample()\n","        else:\n","            # Exploit: Choose the action with the highest Q-value\n","            action = np.argmax(q_table[state, :])\n","\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Update the Q-table using the Q-learning update rule\n","        q_table[state, action] += learning_rate * (\n","                reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action])\n","\n","        state = next_state\n","\n","# Print the learned Q-table\n","print(q_table)\n","\n","# Close the environment\n","env.close()\n"]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","env = gym.make('FrozenLake-v1')\n","\n","num_episodes = 10000\n","learning_rate = 0.1\n","discount_factor = 0.99\n","epsilon = 0.1\n","\n","state_space = env.observation_space.n\n","action_space = env.action_space.n\n","q_table = np.zeros((state_space,action_space))\n","\n","for episode in range(num_episodes):\n","  state = env.reset()\n","  done = False\n","\n","  while not done:\n","    if np.random.uniform(0,1) < epsilon:\n","      action =  env.action_space.sample()\n","    else:\n","      action = np.argmax(q_table[state,:])\n","\n","    next_state,reward,done,_ = env.step(action)\n","    q_table[state,action] += learning_rate*(reward + discount_factor*np.max(q_table[next_state:,]) - q_table[state,action])\n","\n","    state = next_state\n","\n","print(q_table)\n","env.close()"],"metadata":{"id":"8rY-szEtnxtd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684220614842,"user_tz":-330,"elapsed":6016,"user":{"displayName":"ANIMESH GUPTA (RA2011047010021)","userId":"03973304669749828208"}},"outputId":"e7731596-f67a-48cf-ba78-8866366480f9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.40465046 0.40465046 0.40465046 0.40465046]\n"," [0.40465046 0.4046487  0.40464975 0.40465028]\n"," [0.40465046 0.22401029 0.28617607 0.27872084]\n"," [0.35270664 0.         0.         0.0099    ]\n"," [0.40464871 0.40464791 0.40464777 0.40465046]\n"," [0.         0.         0.         0.        ]\n"," [0.40460887 0.18675261 0.09991931 0.05814122]\n"," [0.         0.         0.         0.        ]\n"," [0.4046504  0.2575399  0.27652061 0.23537028]\n"," [0.05093936 0.08280999 0.38744432 0.12867103]\n"," [0.07259798 0.12334422 0.07902425 0.40132052]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.37311322 0.05092412 0.04373919 0.04372226]\n"," [0.03396116 0.40873783 0.         0.01879119]\n"," [0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","# Create the Atari Breakout environment\n","env = gym.make('Breakout-v1')\n","\n","# Set the parameters\n","num_episodes = 1000\n","max_steps = 500\n","epsilon = 1.0\n","epsilon_decay = 0.99\n","gamma = 0.99\n","batch_size = 32\n","\n","# Define the DQN model\n","model = Sequential()\n","model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(84, 84, 4)))\n","model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n","model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(env.action_space.n, activation='linear'))\n","model.compile(loss='mse', optimizer=Adam())\n","\n","# DQN algorithm\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    state = preprocess_state(state)  # Preprocess the state\n","    state = np.stack([state] * 4, axis=2)  # Stack 4 consecutive frames\n","    total_reward = 0\n","    done = False\n","\n","    for step in range(max_steps):\n","        if np.random.rand() <= epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            q_values = model.predict(np.expand_dims(state, axis=0))\n","            action = np.argmax(q_values[0])\n","\n","        next_state, reward, done, _ = env.step(action)\n","        next_state = preprocess_state(next_state)  # Preprocess the next state\n","        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n","\n","        target = reward\n","        if not done:\n","            next_q_values = model.predict(np.expand_dims(next_state, axis=0))\n","            target = reward + gamma * np.max(next_q_values[0])\n","\n","        q_values = model.predict(np.expand_dims(state, axis=0))\n","        q_values[0][action] = target\n","\n","        model.fit(np.expand_dims(state, axis=0), q_values, verbose=0)\n","\n","        state = next_state\n","        total_reward += reward\n","\n","        if done:\n","            break\n","\n","    epsilon *= epsilon_decay\n","\n","    if episode % 100 == 0:\n","        print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n","\n","# Close the environment\n","env.close()\n"],"metadata":{"id":"yu5EueIWTY4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install atari-py==0.2.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQXtcQNyCHVf","executionInfo":{"status":"ok","timestamp":1684216218308,"user_tz":-330,"elapsed":123423,"user":{"displayName":"PRIYANSH SRIVASTAVA (RA2011047010022)","userId":"02639638076610974849"}},"outputId":"983b8009-4935-4c8e-aa03-0833951e1817"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting atari-py==0.2.5\n","  Downloading atari-py-0.2.5.tar.gz (790 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.2/790.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==0.2.5) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atari-py==0.2.5) (1.16.0)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-0.2.5-cp310-cp310-linux_x86_64.whl size=3118540 sha256=c536f4a6d33729bd3cf568983d4e2383634edd8d05d9d827aaed2c6d0e59edcd\n","  Stored in directory: /root/.cache/pip/wheels/8a/cf/91/546c450c42ab0252fbffb1ad265ac51903aca4a53fa46dddb8\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-0.2.5\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RqhtO3m5GhxD"},"execution_count":null,"outputs":[]}]}