{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjC4MXV4NICS","outputId":"c954d6cc-183d-4fc1-feb0-9f81823a37cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[[ 5.6655611   6.73289     5.6655611   6.73289   ]\n","  [ 6.73289     7.811       5.6655611   7.811     ]\n","  [ 1.8932734   8.9         0.85844384  1.13640991]]\n","\n"," [[ 1.67282844  7.811       1.83144887  3.68324788]\n","  [ 6.73289     8.9         6.73289     8.9       ]\n","  [ 1.96891838 10.          2.66852709  5.2705602 ]]\n","\n"," [[ 1.83126381  5.51732074  5.29369585  8.9       ]\n","  [ 7.811       8.9         7.811      10.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}],"source":["import numpy as np\n","\n","# Define the environment\n","env = np.array([[-1, -1, -1],\n","                [-1, -1, -1],\n","                [-1, -1, -1]])\n","\n","# Define the rewards\n","rewards = np.array([[-1, -1, -1],\n","                    [-1, -1, -1],\n","                    [-1, -1, 10]])\n","\n","# Define the Q-Table\n","q_table = np.zeros((env.shape[0], env.shape[1], 4))\n","\n","# Define the hyperparameters\n","alpha = 0.1\n","gamma = 0.99\n","epsilon = 1.0\n","epsilon_decay = 0.99\n","epsilon_min = 0.01\n","episodes = 100000\n","\n","# Define the actions\n","actions = {\n","    0: 'Up',\n","    1: 'Down',\n","    2: 'Left',\n","    3: 'Right'\n","}\n","\n","# Define the Q-Learning algorithm\n","for episode in range(episodes):\n","    state = [0, 0]\n","    done = False\n","    while not done:\n","        # Choose an action\n","        if np.random.random() > epsilon:\n","            action = np.argmax(q_table[state[0], state[1]])\n","        else:\n","            action = np.random.randint(0, 4)\n","        \n","        # Take the action and observe the next state and reward\n","        if actions[action] == 'Up':\n","            next_state = [max(state[0]-1, 0), state[1]]\n","        elif actions[action] == 'Down':\n","            next_state = [min(state[0]+1, 2), state[1]]\n","        elif actions[action] == 'Left':\n","            next_state = [state[0], max(state[1]-1, 0)]\n","        else:\n","            next_state = [state[0], min(state[1]+1, 2)]\n","        \n","        reward = rewards[next_state[0], next_state[1]]\n","        \n","        # Update the Q-Table\n","        q_table[state[0], state[1], action] = (1 - alpha) * q_table[state[0], state[1], action] + alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1]]))\n","        \n","        # Update the state\n","        state = next_state\n","        \n","        # Check if the episode is done\n","        if state == [2, 2]:\n","            done = True\n","        \n","    # Decay the epsilon\n","    if epsilon > epsilon_min:\n","        epsilon *= epsilon_decay\n","\n","# Print the learned Q-Table\n","print(q_table)\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","env = np.array([[-1, -1, -1],\n","                [-1, -1, -1],\n","                [-1, -1, -1]])\n","\n","\n","rewards = np.array([[-1, -1, -1],\n","                    [-1, -1, -1],\n","                    [-1, -1, 10]])\n","\n","q_table = np.zeros((env.shape[0],env.shape[1],4))\n","\n","alpha = 0.1\n","gamma = 0.99\n","epsilon = 1.0\n","epsilon_decay = 0.99\n","epsilon_min = 0.01\n","episodes = 100000\n","\n","actions = {\n","    0: 'Up',\n","    1: 'Down',\n","    2: 'Left',\n","    3: 'Right'\n","}\n","\n","for episode in range(episodes):\n","  state = [0,0]\n","  done = False\n","  while not done:\n","    if np.random.random() > epsilon:\n","      action = np.argmax(q_table[state[0],state[1]])\n","    else:\n","      action = np.random.randint(0,4)\n","\n","    if actions[action] == 'Up':\n","      next_state = [max(state[0]-1,0),state[1]]\n","    elif actions[action] == 'Down':\n","      next_state = [min(state[0]+1,2),state[1]]\n","    elif actions[action] == 'Left':\n","      next_state = [state[0],max(state[1]-1,0)]\n","    else:  \n","      next_state = [state[0],min(state[1]+1,2)]\n","\n","    reward = rewards[next_state[0],next_state[1]]\n","\n","    q_table[state[0], state[1], action] = (1 - alpha) * q_table[state[0], state[1], action] + alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1]]))\n","\n","    state = next_state\n","\n","    if state == [2,2]:\n","      done = True\n","    \n","    if epsilon > epsilon_min:\n","      epsilon *= epsilon_decay\n","\n","print(q_table)"],"metadata":{"id":"elHFNrioNXKn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684222504462,"user_tz":-330,"elapsed":9681,"user":{"displayName":"ANIMESH GUPTA (RA2011047010021)","userId":"03973304669749828208"}},"outputId":"6bcf093a-be29-4cd4-db96-5698af9c1417"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[ 5.6655611   6.73289     5.6655611   6.73289   ]\n","  [-0.38952019  7.811       0.17882716  1.18646984]\n","  [-0.5089312   8.9        -0.28981    -0.29872   ]]\n","\n"," [[ 5.6655611   7.81099999  6.73289     7.811     ]\n","  [ 6.73289     8.9         6.73289     8.9       ]\n","  [ 7.811      10.          7.811       8.9       ]]\n","\n"," [[-0.40672028 -0.48913067  0.42163963  8.9       ]\n","  [ 1.32289271 -0.0829     -0.2267191  10.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pWARHiMmeYtX"},"execution_count":null,"outputs":[]}]}